---
title: "Project 3 - Example Main Script"
author: "Chengliang Tang, Tian Zheng"
output:
  html_document:
    df_print: paged
---

In your final repo, there should be an R markdown file that organizes **all computational steps** for evaluating your proposed image classification framework. 

This file is currently a template for running evaluation experiments of image analysis (or any predictive modeling). You should update it according to your codes but following precisely the same structure. 

```{r}
if(!require("EBImage")){
  source("https://bioconductor.org/biocLite.R")
  biocLite("EBImage")
}

if(!require("xgboost")){
  install.packages("xgboost")
}

library("EBImage")
library("xgboost")
```


### Step 0: specify directories.

Set the working directory to the image folder. Specify the training and the testing set. For data without an independent test/validation set, you need to create your own testing data by random subsampling. In order to obain reproducible results, set.seed() whenever randomization is used. 

```{r wkdir, eval=FALSE}
set.seed(2018)
setwd("C:/Users/zhang/OneDrive/Courseworks/GR5243/Fall2018-Proj3-Sec2--sec2proj3_grp10/doc")
# here replace it with your own path or manually set it in RStudio to where this rmd file is located. 
# use relative path for reproducibility
```

Provide directories for training images. Low-resolution (LR) image set and High-resolution (HR) image set will be in different subfolders. 
```{r}
train_dir <- "../data/train_set/" # This will be modified for different data sets.
train_LR_dir <- paste(train_dir, "LR/", sep="")
train_HR_dir <- paste(train_dir, "HR/", sep="")
train_label_path <- paste(train_dir, "label.csv", sep="") 
```

### Step 1: set up controls for evaluation experiments.

In this chunk, we have a set of controls for the evaluation experiments. 

+ (T/F) cross-validation on the training set
+ (number) K, the number of CV folds
+ (T/F) process features for training set
+ (T/F) run evaluation on an independent test set
+ (T/F) process features for test set

```{r exp_setup}
run.cv=TRUE # run cross-validation on the training set
K <- 10  # number of CV folds
run.feature.train=TRUE # process features for training set
run.test=TRUE # run evaluation on an independent test set
run.feature.test=TRUE # process features for test set
```

Using cross-validation or independent test set evaluation, we compare the performance of models with different specifications. In this example, we use GBM with different `depth`. In the following chunk, we list, in a vector, setups (in this case, `depth`) corresponding to models that we will compare. In your project, you might compare very different classifiers. You can assign them numerical IDs and labels specific to your project. 

```{r model_setup}
model_values1 <- seq(3, 11, 2)
model_values2 <- seq(40, 60, 5)
```

### Step 2: import training images class labels.

We provide extra information of image label: car (0), flower (1), market (2). These labels are not necessary for your model.

```{r train_label}
extra_label <- read.csv(train_label_path, colClasses=c("NULL", NA, NA))
```

### Step 3: construct features and responses

`feature.R` should be the wrapper for all your feature engineering functions and options. The function `feature( )` should have options that correspond to different scenarios for your project and produces an R object that contains features and responses that are required by all the models you are going to evaluate later. 
+ `feature.R`
  + Input: a path for low-resolution images.
  + Input: a path for high-resolution images.
  + Output: an RData file that contains extracted features and corresponding responses

```{r feature}
#source("../lib/feature.R")
feature <- function(LR_dir, HR_dir, n_points=1000){
  
  ### Construct process features for training images (LR/HR pairs)
  
  ### Input: a path for low-resolution images + a path for high-resolution images 
  ###        + number of points sampled from each LR image
  ### Output: an .RData file contains processed features and responses for the images
  
  ### load libraries
  library("EBImage")
  n_files <- 1
  #n_files <- length(list.files(LR_dir))
  
  ### store feature and responses
  featMat <- array(NA, c(n_files * n_points, 8, 3))
  labMat <- array(NA, c(n_files * n_points, 4, 3))
  
  ### read LR/HR image pairs
  
  for(i in 1:n_files){
    imgLR <- readImage(paste0(LR_dir,  "img_", sprintf("%04d", i), ".jpg"))
    imgHR <- readImage(paste0(HR_dir,  "img_", sprintf("%04d", i), ".jpg"))
    ### step 1. sample n_points from imgLR
    #t1 <- Sys.time()
        index.x <- sample(1:dim(imgLR)[2],n_points,replace = TRUE)
        index.y <- sample(1:dim(imgLR)[1],n_points,replace = TRUE)
    #t2 <- Sys.time()
    #print(paste("sample",t2-t1))
    ### step 2. for each sampled point in imgLR,
    
        ### step 2.1. save (the neighbor 8 pixels - central pixel) in featMat
        ###           tips: padding zeros for boundary points
        #pad zero
        b = array(0, dim=c(1,dim(imgLR)[2],3))
        c = array(0, dim=c((dim(imgLR)[1]+2),1,3))
        bimg = abind(b,imgLR,along=1)
        bimgb = abind(bimg,b,along=1)
        cbimgb = abind(c,bimgb,along=2)
        cbimgbc = abind(cbimgb,c,along=2)
    #t3 <- Sys.time()   
    #print(paste("pad",t3-t2))
        mat <- matrix(1:n_points, ncol = 1)
        func1 <- function(j){
          cbimgbc[(index.y[j]):(index.y[j]+2),(index.x[j]):(index.x[j]+2),]
        }
        points1 <- apply(mat,1,func1)
    #t4 <- Sys.time()
    #print(paste("9points",t4-t3))
        # neighbor 8 pixels - central pixel

        func2 <- function(k){
        allnei = points1[-c(5,14,23),k]
        allcen = points1[c(5,14,23),k]
        featmat = allnei - rep(allcen,c(8,8,8))
        }
        featmat <- apply(mat,1, func2)
    #t5 <- Sys.time()
    #print(paste("vectorize",t5-t4))
       ### step 2.2. save the corresponding 4 sub-pixels of imgHR in labMat
    imgHR <- imgHR@.Data
     func3 <- function(m){
      imgHR[(2*index.y[m]-1):(2*index.y[m]),(2*index.x[m]-1):(2*index.x[m]),]
     }
     points.hr <- apply(mat,1,func3)
     func4 <- function(k){
       newnei = points.hr[,k]
       allcen = points1[c(5,14,23),k]
       points.hr = newnei - rep(allcen,c(4,4,4))
     }
     points.hr <- apply(mat,1, func4)
    #t6 <- Sys.time()  
    #print(paste("hrpoints",t6-t5))
    ### step 3. repeat above for three channels
     
     featMat[(1+(i-1)*n_points):(n_points*i),,] <- array(t(featmat),dim=c(n_points,8,3))
     labMat[(1+(i-1)*n_points):(n_points*i),,] <-  array(t(points.hr),dim=c(n_points,4,3))
    
    #t7 <- Sys.time()
    #print(paste("toarray",t7-t6))
  }
  
  return(list(feature = featMat, label = labMat))
}
tm_feature_train <- NA
if(run.feature.train){
  tm_feature_train <- system.time(dat_train <- feature(train_LR_dir, train_HR_dir))
  feat_train <- dat_train$feature
  label_train <- dat_train$label
}
# save.image(file="../output/feature_train.RData")
dim(feat_train)
dim(label_train)
```


### Step 4: Train a classification model with training images
Call the train model and test model from library. 

`train.R` and `test.R` should be wrappers for all your model training steps and your classification/prediction steps. 
+ `train.R`
  + Input: a path that points to the training set features and responses.
  + Output: an RData file that contains trained classifiers in the forms of R objects: models/settings/links to external trained configurations.
+ `test.R`
  + Input: a path that points to the test set features.
  + Input: an R object that contains a trained classifier.
  + Output: an R object of response predictions on the test set. If there are multiple classifiers under evaluation, there should be multiple sets of label predictions. 
```{r loadlib}
#source("../lib/train_xgboost.R")
#source("../lib/test_xgboost.R")
#########################################################
### Train a classification model with training features ###
#########################################################

### Author: Chengliang Tang
### Project 3


train <- function(dat_train, label_train, par=NULL){
  
  ### Train a Gradient Boosting Model (GBM) using processed features from training images
  
  ### Input: 
  ###  -  features from LR images 
  ###  -  responses from HR images
  ### Output: a list for trained models
  
  ### load libraries
  library("xgboost")
  
  ### creat model list
  modelList <- list()
  
  ### Train with gradient boosting model
  if(is.null(par)){
    depth <- 3
    nrounds <- 40
  } else {
    depth <- par$depth
    nrounds <- par$nrounds
  }
  
  ### the dimension of response arrat is * x 4 x 3, which requires 12 classifiers
  ### this part can be parallelized
  for (i in 1:12){
    ## calculate column and channel
    c1 <- (i-1) %% 4 + 1
    c2 <- (i-c1) %/% 4 + 1
    featMat <- dat_train[, , c2]
    labMat <- label_train[,c1, c2]
    
    fit_xgb <- xgboost(data = featMat, 
                       label = labMat, 
                       eta = 0.1,
                       max_depth = depth, 
                       
                       nrounds = nrounds,
                       verbose = 0)
    modelList[[i]] <- list(fit=fit_xgb)
  }
  
 
  return(modelList)
}
```

```{r}
######################################################
### Fit the regression model with testing data ###
######################################################

### Author: Chengliang Tang
### Project 3

test <- function(modelList, dat_test){
  
  ### Fit the classfication model with testing data
  
  ### Input: 
  ###  - the fitted classification model list using training data
  ###  - processed features from testing images 
  ### Output: training model specification
  
  ### load libraries
  library("xgboost")
  
  predArr <- array(NA, c(dim(dat_test)[1], 4, 3))
  
  for (i in 1:12){
    fit_train <- modelList[[i]]
    ### calculate column and channel
    c1 <- (i-1) %% 4 + 1
    c2 <- (i-c1) %/% 4 + 1
    featMat <- dat_test[, , c2]
    ### make predictions
    predArr[, c1, c2] <- predict(fit_train$fit, newdata=featMat, 
                                  type="response")
  }
  return(predArr)
}
```

#### Model selection with cross-validation
* Do model selection by choosing among different values of training model parameters, that is, the interaction depth for XGB in this example. 
```{r runcv, message=FALSE, warning=FALSE, include=FALSE}
#source("../lib/cross_validation_xgboost.R")
########################
### Cross Validation ###
########################

### Author: Chengliang Tang
### Project 3

cv.function <- function(X.train, y.train, depth, nrounds, K){
  
  n <- dim(y.train)[1]
  n.fold <- floor(n/K)
  s <- sample(rep(1:K, c(rep(n.fold, K-1), n-(K-1)*n.fold)))  
  cv.error <- rep(NA, K)
  
  for (i in 1:K){
    train.data <- X.train[s != i, ,]
    train.label <- y.train[s != i, ,]
    test.data <- X.train[s == i, ,]
    test.label <- y.train[s == i, ,]
    
    par <- list(depth=depth, nrounds= nrounds)
    fit <- train(train.data, train.label, par)
    pred <- test(fit, test.data)  
    cv.error[i] <- mean((pred - test.label)^2)  
    
  }			
  return(c(mean(cv.error),sd(cv.error)))
}

if(run.cv){
  err_cv <- array(dim=c(length(model_values1)*length(model_values2), 2))
  for(k1 in 1:length(model_values1)){
    for(k2 in 1:length(model_values2)){ 
    cat("k1=", k1, ",k2=", k2, "\n")
    index <- (k1-1)*length(model_values2) + k2 
    err_cv[index,] <- cv.function(feat_train, label_train, model_values1[k1], model_values2[k2], K)
    }
  }

  #save(err_cv, file="../output/xgb_err_cv.RData")
}
```

* Choose the "best" parameter value
```{r}
err_cv  <- data.frame(err_cv)
colnames(err_cv) <- c("cv_mean","cv_sd")
par_mix <- expand.grid(model_values1, model_values2)

colnames(par_mix) <- c("K1","K2")
par_mix <- par_mix[order(par_mix$K1),]
err_cv  <- cbind(err_cv, par_mix)
plot(err_cv[,"K2"], err_cv[,1], xlab="model value combination", ylab="CV Error",
       main="Cross Validation Error", type="n", ylim=c(0.003, 0.005))
points(err_cv[,"K2"], err_cv[,1], col="blue", pch=16)
lines(err_cv[,"K2"], err_cv[,1], col="blue")
arrows(err_cv[,"K2"], err_cv[,1]-err_cv[,2], err_cv[,"K2"], err_cv[,1]+err_cv[,2], 
        length=0.1, angle=90, code=3)
```
```{r}
err_cv
```
```{r best_model}
model_best <- NULL 
if(run.cv){
  model_best <- err_cv[which.min(err_cv[,1]),]
}
par_best <- list(depth = model_best$K1, nrounds = model_best$K2)
par_best
```

* Train the model with the entire training set using the selected model (model parameter) via cross-validation.
```{r final_train, include=FALSE}
tm_train=NA
tm_train <- system.time(fit_train <- train(feat_train, label_train, par = par_best))
save(fit_train, file="../output/fit_train_xgboost.RData")
```

```{r}
summary(fit_train[[1]]$fit)
```

### Step 5: Super-resolution for test images
Feed the final training model with the completely holdout testing data. 
+ `superResolution.R`
  + Input: a path that points to the folder of low-resolution test images.
  + Input: a path that points to the folder (empty) of high-resolution test images.
  + Input: an R object that contains tuned predictors.
  + Output: construct high-resolution versions for each low-resolution test image.
```{r superresolution}
#source("../lib/superResolution.R")
########################
### Super-resolution ###
########################

### Author: Chengliang Tang
### Project 3
#source("../lib/superResolution.R")
test_dir <- "../data/test_set/" # This will be modified for different data sets.
test_LR_dir <- paste(test_dir, "LR/", sep="")
test_HR_dir <- paste(test_dir, "HR/", sep="")
# modelList = fit_train
# LR_dir <- test_LR_dir
# HR_dir <- test_HR_dir
superResolution <- function(LR_dir, HR_dir, modelList){
  
  
  ### Construct high-resolution images from low-resolution images with trained predictor
  
  ### Input: a path for low-resolution images + a path for high-resolution images 
  ###        + a list for predictors
  
  ### load libraries
  library("EBImage")
  n_files <- length(list.files(LR_dir))
  
  ### read LR/HR image pairs
  for (i in 1:n_files){
    imgLR <- readImage(paste0(LR_dir,  "img", "_", sprintf("%04d", i), ".jpg"))
    pathHR <- paste0(HR_dir,  "img", "_", sprintf("%04d", i), ".jpg")
    featMat <- array(NA, c(dim(imgLR)[1] * dim(imgLR)[2], 8, 3))
    
    ### step 1. for each pixel and each channel in imgLR:
    ###           save (the neighbor 8 pixels - central pixel) in featMat
    ###           tips: padding zeros for boundary points
    b = array(0, dim=c(1,dim(imgLR)[2],3))
    c = array(0, dim=c((dim(imgLR)[1]+2),1,3))
    bimg = abind(b,imgLR,along=1)
    bimgb = abind(bimg,b,along=1)
    cbimgb = abind(c,bimgb,along=2)
    cbimgbc = abind(cbimgb,c,along=2)
    #t3 <- Sys.time()   
    #print(paste("pad",t3-t2))
    index.x <- (1:dim(imgLR)[2])
    index.y <- (1:dim(imgLR)[1])
    index.all <- expand.grid(index.y,index.x)
    
    n_points = dim(imgLR)[1] * dim(imgLR)[2]
    mat <- matrix(1:n_points,ncol = 1)
    
    func1 <- function(j){
      cbimgbc[(index.all[j,1]):(index.all[j,1]+2),(index.all[j,2]):(index.all[j,2]+2),]
    }
    points1 <- apply(mat,1,func1)
    
    
    func2 <- function(k){
      allnei = points1[-c(5,14,23),k]
      allcen = points1[c(5,14,23),k]
      featmat = allnei - rep(allcen,c(8,8,8))
    }
    featmat <- apply(mat,1, func2)
    
    
    func3 <- function(m){
      allcen = points1[c(5,14,23),m]
      allcen = rep(allcen,c(4,4,4))
    }
    
    allcen <- apply(mat,1,func3)
    allcen <- t(allcen)
    
    
    featMat <- array(t(featmat),dim=c(n_points,8,3))
    
    
    ### step 2. apply the modelList over featMat
    predMat <- test(modelList, featMat)
    predMat[,,1] <- predMat[,,1] + allcen[,1:4]
    predMat[,,2] <- predMat[,,2] + allcen[,5:8]
    predMat[,,3] <-  predMat[,,3] + allcen[,9:12]
    
    predall <- array(NA,dim=c(dim(imgLR)[1]*2,dim(imgLR)[2]*2,3))
    dim(predall)
    for (k in 1:3){
      for (j in seq(from = 1, to = dim(imgLR)[2]*2,by = 2)){
        for (i in seq(from = 1, to = dim(imgLR)[1]*2,by = 2)){
          indexmat = ceiling(i/2)+(ceiling(j/2)-1)*dim(imgLR)[1]
          predall[(i:(i+1)),(j:(j+1)),k] <-rbind(predMat[indexmat,1:2,k],predMat[indexmat,3:4,k])
        }
        
      }
    }
    length(which(is.na(predall[,,1])))
    predall1 = Image(predall, colormode=Color)
    
    ### step 3. recover high-resolution from predMat and save in HR_dir
    # save(predall, file="../data/test_set/HR2/img_0001.jpg") # no pic
    writeImage(predall1, files = pathHR)
  }
}

tm_test=NA
run.test=TRUE # run evaluation on an independent test set
if(run.test){
  load(file="../output/fit_train_xgboost.RData")
  tm_test <- system.time(superResolution(test_LR_dir, test_HR_dir, fit_train))
}
tm_test
```

### Summarize Running Time
Prediction performance matters, so does the running times for constructing features and for training the model, especially when the computation resource is limited. 
```{r running_time}
cat("Time for constructing training features=", tm_feature_train[1], "s \n")
# cat("Time for constructing testing features=", tm_feature_test[1], "s \n")
cat("Time for training model=", tm_train[1], "s \n")
cat("Time for super-resolution=", tm_test[1], "s \n")
```

